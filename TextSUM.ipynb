{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ABTBb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ABTBb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Extractive Text Summarization\n",
    "As the name implies, extractive text summarizing significant information from enormous amounts of text and arranges it into clear and succinct summaries. The approach is simple in that it extracts texts based on factors such the text to be summarized, the most essential sentences (Top K), and the importance of each of these phrases to the overall subject. This, however, implies that the approach is constrained to specified parameters, which might lead to biased retrieved text under certain scenarios.\n",
    "Extractive text summarizing is the most often utilized approach by automated text summarizers due to its simplicity in most use scenarios.\n",
    "\n",
    "Abstractive Text Summarization\n",
    "Abstractive text summarization creates readable sentences from the complete text input. Large volumes of text are rewritten by producing acceptable representations, which are then analyzed and summarized using natural language processing. What distinguishes this technology is its almost AI-like capacity to parse text utilizing a machineâ€™s semantic capabilities and iron out wrinkles using NLP.\n",
    "Although it is not as straightforward to utilize as the extractive technique, abstract summary is significantly more beneficial in many cases. In many ways, it is a forerunner to full-fledged AI authoring tools. This is not to say that extractive summarization is unnecessary.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting text in words\n",
    "words = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the frequency of each word which is not a stopWord\n",
    "wordFreq = dict()\n",
    "for word in words:\n",
    "    word = word.lower()\n",
    "    if word in stopwords:\n",
    "        continue\n",
    "    if word in wordFreq:\n",
    "        wordFreq[word] += 1\n",
    "    else:\n",
    "        wordFreq[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sliptting text in sentences\n",
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating importance of sentence => checking sentences with words which have high freqency\n",
    "def getSentenceImp():\n",
    "    sentImp = dict()\n",
    "    for sentence in sentences:\n",
    "        sentImp[sentence] = 0\n",
    "        for word, freq in wordFreq.items():\n",
    "            if word in sentence.lower():\n",
    "                sentImp[sentence] += freq\n",
    "            else:\n",
    "                sentImp[sentence] = freq\n",
    "    return sentImp\n",
    "\n",
    "sentenceImportance = getSentenceImp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating average of all the sentence importance in the dictonay\n",
    "def getSentAvg():\n",
    "    total = 0\n",
    "    for sentence in sentenceImportance:\n",
    "        total += sentenceImportance[sentence]\n",
    "        avg = int(total / len(sentenceImportance))\n",
    "    return avg\n",
    "\n",
    "average = getSentAvg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  This is not to say that extractive summarization is unnecessary. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating summary by sorting\n",
    "summary = ''\n",
    "for sentence in sentences:\n",
    "    if (sentence in sentenceImportance) and (sentenceImportance[sentence] > 1.2 * average):\n",
    "        summary += \" \" + sentence\n",
    "\n",
    "print(\"\\n\\n\", summary, \"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
